# Awesome-Cross-modal-Reasoning-with-Large-Language-Models






# Awesome Papers

## Multimodal Fusion Engine

### Prompt Tuning


|  Model  |   Paper  |   Code   |   Demo   |   Year   |
|:--------|:--------:|:--------:|:--------:|:--------:|
|  CAT  ![Star](https://img.shields.io/github/stars/ttengwang/Caption-Anything.svg?style=social&label=Star) | <br> [**Caption Anything: Interactive Image Description with Diverse Multimodal Controls**](https://arxiv.org/pdf/2305.02677) <br>  | [Github](https://github.com/ttengwang/Caption-Anything) | [Demo](https://huggingface.co/spaces/TencentARC/Caption-Anything) |2023|
|  KAT ![Star](https://img.shields.io/github/stars/guilk/KAT.svg?style=social&label=Star)  | <br> [**KAT: A Knowledge Augmented Transformer for Vision-and-Language**](https://arxiv.org/abs/2112.08614) <br>  | [Github](https://github.com/guilk/KAT) | -- |2022|
| REVIVE  ![Star](https://img.shields.io/github/stars/yuanze-lin/REVIVE.svg?style=social&label=Star)  | <br> [**REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering**](https://arxiv.org/abs/2206.01201) <br>  | [Github](https://github.com/yuanze-lin/REVIVE) | -- |2022|
|  Visual ChatGPT ![Star](https://img.shields.io/github/stars/microsoft/TaskMatrix.svg?style=social&label=Star)  | <br>[**Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models**](https://arxiv.org/pdf/2303.04671.pdf)<br>  | [Github](https://github.com/microsoft/TaskMatrix) | [Demo](https://huggingface.co/spaces/microsoft/visual_chatgpt) |2023|
|  PaLM-E ![Star](https://img.shields.io/github/stars/kyegomez/PALM-E.svg?style=social&label=Star) | <br> [**PaLM-E: An Embodied Multimodal Language Model**](https://arxiv.org/pdf/2303.03378.pdf) <br>  | [Github](https://github.com/kyegomez/PALM-E) | [Demo](https://palm-e.github.io/#demo) |2022|
|  VL-T5  ![Star](https://img.shields.io/github/stars/j-min/VL-T5.svg?style=social&label=Star) | <br> [**Unifying Vision-and-Language Tasks via Text Generation**](https://arxiv.org/abs/2102.02779) <br>  | [Github](https://github.com/j-min/VL-T5) | [Demo](https://replicate.com/j-min/vl-t5) |2021|
| BLIP-2   ![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) | <br> [**BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models**](https://arxiv.org/pdf/2301.12597.pdf) <br>  | [Github](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) | [Demo](https://colab.research.google.com/github/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb) |2023|
| VPGTrans ![Star](https://img.shields.io/github/stars/VPGTrans/VPGTrans.svg?style=social&label=Star)  | <br> [**VPGTrans: Transfer Visual Prompt Generator across LLMs**](https://proceedings.neurips.cc/paper_files/paper/2023/hash/407106f4b56040b2e8dcad75a6e461e5-Abstract-Conference.html) <br>  | [Github](https://github.com/VPGTrans/VPGTrans) | [Demo](https://ee569fe29733644a33.gradio.live/) |2023|
| eP-ALM ![Star](https://img.shields.io/github/stars/mshukor/eP-ALM.svg?style=social&label=Star)  | <br> [**eP-ALM: Efficient Perceptual Augmentation of Language Models**](https://openaccess.thecvf.com/content/ICCV2023/papers/Shukor_eP-ALM_Efficient_Perceptual_Augmentation_of_Language_Models_ICCV_2023_paper.pdf) <br>  | [Github](https://github.com/mshukor/eP-ALM) | [Demo](https://huggingface.co/mshukor) |2023|
| VCOT | <br> [**Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings**](https://arxiv.org/pdf/2305.02317.pdf)<br>  | [Github](https://github.com/dannyrose30/VCOT) | -- |2024|
| VCoder ![Star](https://img.shields.io/github/stars/SHI-Labs/VCoder.svg?style=social&label=Star)  | <br> [**VCoder: Versatile Vision Encoders for Multimodal Large Language Models**](https://openaccess.thecvf.com/content/CVPR2024/html/Jain_VCoder_Versatile_Vision_Encoders_for_Multimodal_Large_Language_Models_CVPR_2024_paper.html) <br>  | [Github](https://github.com/SHI-Labs/VCoder) | [Demo](https://huggingface.co/shi-labs/vcoder_ds_llava-v1.5-13b) |2024|
|  V2L-Tokenizer  ![Star](https://img.shields.io/github/stars/zh460045050/V2L-Tokenizer.svg?style=social&label=Star) | <br> [**Beyond Text: Frozen Large Language Models in Visual Signal Comprehension**](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_Beyond_Text_Frozen_Large_Language_Models_in_Visual_Signal_Comprehension_CVPR_2024_paper.pdf) <br>  | [Github](https://github.com/zh460045050/V2L-Tokenizer) | -- |2024|
| BT-Adapter ![Star](https://img.shields.io/github/stars/farewellthree/BT-Adapter.svg?style=social&label=Star)  | <br> [**BT-Adapter: Video Conversation is Feasible Without Video Instruction Tuning**](https://arxiv.org/abs/2309.15785) <br>  | [Github](https://github.com/farewellthree/BT-Adapter) | [Demo](https://huggingface.co/farewellthree/BTAdapter-Weight) |2024|
|  MEAgent | <br> [**Few-Shot Multimodal Explanation for Visual Question Answering**](https://openreview.net/forum?id=jPpK9RzWvh&referrer=%5Bthe%20profile%20of%20Dizhan%20Xue%5D(%2Fprofile%3Fid%3D~Dizhan_Xue1)) <br>  | -- | -- |2024|

### Instruction Tuning

|  Model  |   Paper  |   Code   |   Demo   |   Year   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| LION ![Star](https://img.shields.io/github/stars/JiuTian-VL/JiuTian-LION.svg?style=social&label=Star) | <br> [**LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge**](https://arxiv.org/abs/2311.11860) <br>  | [Github](https://github.com/JiuTian-VL/JiuTian-LION) | -- |2024|
| VistaLLM | <br> [**Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model**](https://arxiv.org/abs/2312.12423) <br>  | [Github](https://shramanpramanick.github.io/VistaLLM/) | -- |2024|
|  Chat-UniVi ![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Chat-UniVi.svg?style=social&label=Star) | <br> [**Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding**](https://arxiv.org/abs/2305.13225) <br>  | [Github](https://github.com/PKU-YuanGroup/Chat-UniVi) | [Demo](https://huggingface.co/spaces/Chat-UniVi/Chat-UniVi) |2024|
|  GPT4RoI ![Star](https://img.shields.io/github/stars/jshilong/GPT4RoI.svg?style=social&label=Star) | <br> [**Gpt4roi: Instruction tuning large language model on region-of-interest**](https://arxiv.org/abs/2307.03601) <br>  | [Github](https://github.com/jshilong/GPT4RoI) | [Demo](http://139.196.83.164:7000/) |2024|
| LLaVA ![Star](https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&label=Star)  | <br> [**Visual instruction tuning**](https://proceedings.neurips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html) <br>  | [Github](https://llava-vl.github.io/) | [Demo](https://llava.hliu.cc/) |2023|
| InstructBLIP ![Star](https://img.shields.io/github/stars/salesforce/LAVIS/tree/main/projects/instructblip.svg?style=social&label=Star)  | <br> [**InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning**](https://arxiv.org/abs/2305.06500) <br>  | [Github](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip) | -- |2023|
| Video-ChatGPT  ![Star](https://img.shields.io/github/stars/mbzuai-oryx/Video-ChatGPT.svg?style=social&label=Star)  | <br> [**Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models**](https://arxiv.org/abs/2306.05424) <br>  | [Github](https://github.com/mbzuai-oryx/Video-ChatGPT) | [Demo](https://www.ival-mbzuai.com/video-chatgpt) |2024|
| LLaVA-Med ![Star](https://img.shields.io/github/stars/microsoft/LLaVA-Med.svg?style=social&label=Star)  | <br> [**LLaVA-Med: Large Language and Vision Assistant for Biomedicine**](https://arxiv.org/abs/2306.00890) <br>  | [Github](https://github.com/microsoft/LLaVA-Med) | [Demo](https://huggingface.co/microsoft/llava-med-v1.5-mistral-7b) |2023|
| MedVInt ![Star](https://img.shields.io/github/stars/xiaoman-zhang/PMC-VQA.svg?style=social&label=Star)  | <br> [**Pmc-vqa: Visual instruction tuning for medical visual question**](https://arxiv.org/abs/2305.10415) <br>  | [Github](https://github.com/xiaoman-zhang/PMC-VQA) | [Demo](https://huggingface.co/xmcmic/MedVInT-TE/) |2023|
| Gpt4tools ![Star](https://img.shields.io/github/stars/AILab-CVC/GPT4Tools.svg?style=social&label=Star)  | <br> [**Gpt4tools: Teaching large language model to use tools via self-instruction**](https://arxiv.org/abs/2305.18752) <br>  | [Github](https://github.com/AILab-CVC/GPT4Tools) | [Demo](https://huggingface.co/stevengrove/gpt4tools-vicuna-13b-lora) |2023|
|  MiniGPT-5 ![Star](https://img.shields.io/github/stars/eric-ai-lab/MiniGPT-5.svg?style=social&label=Star) | <br> [**MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens**](https://arxiv.org/abs/2310.02239) <br>  | [Github](https://github.com/eric-ai-lab/MiniGPT-5) | -- |2024|
| MiniGPT-4 ![Star](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&label=Star)  | <br> [**MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models**](https://arxiv.org/abs/2304.10592) <br>  | [Github](https://github.com/Vision-CAIR/MiniGPT-4) | [Demo](https://huggingface.co/Vision-CAIR/MiniGPT-4) |2023|
| MiniGPT-v2 ![Star](https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&label=Star)  | <br> [**MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning**](https://arxiv.org/abs/2310.09478) <br>  | [Github](https://github.com/Vision-CAIR/MiniGPT-4) | [Demo](https://minigpt-v2.github.io/) |2023|
| VideoChat ![Star](https://img.shields.io/github/stars/OpenGVLab/Ask-Anything.svg?style=social&label=Star)  | <br> [**VideoChat: Chat-Centric Video Understanding**](https://arxiv.org/abs/2305.06355) <br>  | [Github](https://github.com/OpenGVLab/Ask-Anything) | [Demo](https://openxlab.org.cn/apps/detail/yinanhe/VideoChat2) |2023|
|  LaVIN ![Star](https://img.shields.io/github/stars/luogen1996/LaVIN.svg?style=social&label=Star) | <br> [**Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models**](https://arxiv.org/abs/2305.15023)) <br>  | [Github](https://github.com/luogen1996/LaVIN) | -- |2023|
|  Video-LLaMA ![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA.svg?style=social&label=Star) | <br> [**Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding**](https://arxiv.org/abs/2306.02858) <br>  | [Github](https://github.com/DAMO-NLP-SG/Video-LLaMA) | [Demo](https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA) |2023|
|  DetGPT ![Star](https://img.shields.io/github/stars/OptimalScale/DetGPT.svg?style=social&label=Star) | <br> [**DetGPT: Detect What You Need via Reasoning**](https://arxiv.org/abs/2305.14167) <br>  | [Github](https://github.com/OptimalScale/DetGPT) | [Demo](https://a03e18d54fcb7ceb54.gradio.live/) |2023|
|  Macaw-LLM ![Star](https://img.shields.io/github/stars/lyuchenyang/Macaw-LLM.svg?style=social&label=Star) | <br> [**Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration**](https://arxiv.org/abs/2306.09093) <br>  | [Github](https://github.com/lyuchenyang/Macaw-LLM) | [Demo](https://www.dropbox.com/scl/fo/4ded7qj8my90fes1yxqqd/h?dl=0&rlkey=is2zkfrw76yiidwolgm47x9tv) |2023|
|  LLaMA-Adapter ![Star](https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter.svg?style=social&label=Star) | <br> [**LLaMA-Adapter: Efficient Fine-tuning of LLaMA**](https://arxiv.org/abs/2303.16199) <br>  | [Github](https://github.com/OpenGVLab/LLaMA-Adapter) | [Demo](http://imagebind-llm.opengvlab.com/) |2023|
|  LLaMA-Adapter V2 ![Star](https://img.shields.io/github/stars/Alpha-VLLM/LLaMA2-Accessory.svg?style=social&label=Star) | <br> [**LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model**](https://arxiv.org/abs/2304.15010) <br>  | [Github](https://github.com/Alpha-VLLM/LLaMA2-Accessory) | [Demo](http://imagebind-llm.opengvlab.com/) |2023|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|




| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|

| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [****]() <br>  | [Github]() | [Demo]() |20|
