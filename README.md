# Awesome-Cross-modal-Reasoning-with-Large-Language-Models






# Awesome Papers

## Multimodal Fusion Engine

### Prompt Tuning


|  Model  |   Paper  |   Code   |   Demo   |   Year   |
|:--------|:--------:|:--------:|:--------:|:--------:|
| ![Star](https://img.shields.io/github/stars/ttengwang/Caption-Anything.svg?style=social&label=Star) CAT | <br> [**Caption Anything: Interactive Image Description with Diverse Multimodal Controls**](https://arxiv.org/pdf/2305.02677) <br>  | [Github](https://github.com/ttengwang/Caption-Anything) | [Demo](https://huggingface.co/spaces/TencentARC/Caption-Anything) |2023|
| ![Star](https://img.shields.io/github/stars/guilk/KAT.svg?style=social&label=Star) KAT | <br> [**KAT: A Knowledge Augmented Transformer for Vision-and-Language**](https://arxiv.org/abs/2112.08614) <br>  | [Github](https://github.com/guilk/KAT) | -- |2022|
| ![Star](https://img.shields.io/github/stars/yuanze-lin/REVIVE.svg?style=social&label=Star) REVIVE | <br> [**REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering**](https://arxiv.org/abs/2206.01201) <br>  | [Github](https://github.com/yuanze-lin/REVIVE) | -- |2022|
| ![Star](https://img.shields.io/github/stars/microsoft/TaskMatrix.svg?style=social&label=Star) Visual ChatGPT  | <br>[**Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models**](https://arxiv.org/pdf/2303.04671.pdf)<br>  | [Github](https://github.com/microsoft/TaskMatrix) | [Demo](https://huggingface.co/spaces/microsoft/visual_chatgpt) |2023|
| ![Star](https://img.shields.io/github/stars/kyegomez/PALM-E.svg?style=social&label=Star) PaLM-E | <br> [**PaLM-E: An Embodied Multimodal Language Model**](https://arxiv.org/pdf/2303.03378.pdf) <br>  | [Github](https://github.com/kyegomez/PALM-E) | [Demo](https://palm-e.github.io/#demo) |2022|
| ![Star](https://img.shields.io/github/stars/j-min/VL-T5.svg?style=social&label=Star)VL-T5  | <br> [**Unifying Vision-and-Language Tasks via Text Generation**](https://arxiv.org/abs/2102.02779) <br>  | [Github](https://github.com/j-min/VL-T5) | [Demo](https://replicate.com/j-min/vl-t5) |2021|
| ![Star](https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&label=Star) BLIP-2 | <br> [**BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models**](https://arxiv.org/pdf/2301.12597.pdf) <br>  | [Github](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) | [Demo](https://colab.research.google.com/github/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb) |2023|
| ![Star](https://img.shields.io/github/stars/VPGTrans/VPGTrans.svg?style=social&label=Star) VPGTrans | <br> [**VPGTrans: Transfer Visual Prompt Generator across LLMs**](https://proceedings.neurips.cc/paper_files/paper/2023/hash/407106f4b56040b2e8dcad75a6e461e5-Abstract-Conference.html) <br>  | [Github](https://github.com/VPGTrans/VPGTrans) | [Demo](https://ee569fe29733644a33.gradio.live/) |2023|
| ![Star](https://img.shields.io/github/stars/mshukor/eP-ALM.svg?style=social&label=Star) eP-ALM | <br> [**eP-ALM: Efficient Perceptual Augmentation of Language Models**](https://openaccess.thecvf.com/content/ICCV2023/papers/Shukor_eP-ALM_Efficient_Perceptual_Augmentation_of_Language_Models_ICCV_2023_paper.pdf) <br>  | [Github](https://github.com/mshukor/eP-ALM) | [Demo](https://huggingface.co/mshukor) |2023|
| VCOT | <br> [**Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings**](https://arxiv.org/pdf/2305.02317.pdf)<br>  | [Github](https://github.com/dannyrose30/VCOT) | -- |2024|
| ![Star](https://img.shields.io/github/stars/SHI-Labs/VCoder.svg?style=social&label=Star)  | <br> [**VCoder: Versatile Vision Encoders for Multimodal Large Language Models**](https://openaccess.thecvf.com/content/CVPR2024/html/Jain_VCoder_Versatile_Vision_Encoders_for_Multimodal_Large_Language_Models_CVPR_2024_paper.html) <br>  | [Github](https://github.com/SHI-Labs/VCoder) | [Demo](https://huggingface.co/shi-labs/vcoder_ds_llava-v1.5-13b) |2024|
| ![Star](https://img.shields.io/github/stars/zh460045050/V2L-Tokenizer.svg?style=social&label=Star) V2L-Tokenizer| <br> [**Beyond Text: Frozen Large Language Models in Visual Signal Comprehension**](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_Beyond_Text_Frozen_Large_Language_Models_in_Visual_Signal_Comprehension_CVPR_2024_paper.pdf) <br>  | [Github](https://github.com/zh460045050/V2L-Tokenizer) | -- |2024|
| ![Star](https://img.shields.io/github/stars/farewellthree/BT-Adapter.svg?style=social&label=Star) BT-Adapter| <br> [**BT-Adapter: Video Conversation is Feasible Without Video Instruction Tuning**](https://arxiv.org/abs/2309.15785) <br>  | [Github](https://github.com/farewellthree/BT-Adapter) | [Demo](https://huggingface.co/farewellthree/BTAdapter-Weight) |2024|



| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [** **](   ) <br>  | [Github](   ) | [Demo](  ) |20-|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [** **](   ) <br>  | [Github](   ) | [Demo](  ) |20-|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [** **](   ) <br>  | [Github](   ) | [Demo](  ) |20-|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [** **](   ) <br>  | [Github](   ) | [Demo](  ) |20-|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [** **](   ) <br>  | [Github](   ) | [Demo](  ) |20-|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [** **](   ) <br>  | [Github](   ) | [Demo](  ) |20-|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [** **](   ) <br>  | [Github](   ) | [Demo](  ) |20-|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [** **](   ) <br>  | [Github](   ) | [Demo](  ) |20-|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [** **](   ) <br>  | [Github](   ) | [Demo](  ) |20-|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [** **](   ) <br>  | [Github](   ) | [Demo](  ) |20-|
| ![Star](https://img.shields.io/github/stars/***************.svg?style=social&label=Star)  | <br> [** **](   ) <br>  | [Github](   ) | [Demo](  ) |20-|



|  <br>  <br> | arXiv | -08 | [Github]() | [Demo]() |


| ![Star](https://img.shields.io/github/stars/)  | <br> [** **](   ) <br>  | [Github](   ) | [Demo](  ) |20--|






| ![Star](https://img.shields.io/github/stars/ttengwang/Caption-Anything.svg?style=social&label=Star) <br> [**Caption Anything: Interactive Image Description with Diverse Multimodal Controls**](https://arxiv.org/pdf/2305.02677.pdf) <br> | arXiv | 2023-05-04 | [Github](https://github.com/ttengwang/Caption-Anything) | [Demo](https://huggingface.co/spaces/TencentARC/Caption-Anything) |



| ![Star](https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&label=Star) <br> [**mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models**](https://www.arxiv.org/pdf/2408.04840) <br> | arXiv | 2024-08-09 | [Github](https://github.com/X-PLUG/mPLUG-Owl) | - |
| ![Star](https://img.shields.io/github/stars/VITA-MLLM/VITA.svg?style=social&label=Star) <br> [**VITA: Towards Open-Source Interactive Omni Multimodal LLM**](https://arxiv.org/pdf/2408.05211) <br> | arXiv | 2024-08-09 | [Github](https://github.com/VITA-MLLM/VITA) | - | 
| ![Star](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT.svg?style=social&label=Star) <br> [**LLaVA-OneVision: Easy Visual Task Transfer**](https://arxiv.org/pdf/2408.03326) <br> | arXiv | 2024-08-06 | [Github](https://github.com/LLaVA-VL/LLaVA-NeXT) | [Demo](https://llava-onevision.lmms-lab.com) | 



